Prompt Performance Analytics - Product Brief
1. Executive Summary
The Prompt Performance Analytics application is a two-agent system designed for analyzing, scoring, and improving AI prompts. The core idea is simple: as humans and enterprise multi-agent systems formulate prompts, optimizing them for clarity, token efficiency, and goal alignment is essential for cost savings and better AI outputs. This system automates the evaluation process, scores prompts along multiple dimensions, identifies common mistakes, provides optimized rewrites, and maintains an enterprise-wide view of prompt quality through an analytics dashboard.

2. What the Agent Does
The system operates through two distinct agents that perform the following tasks:

Agent 1: Prompt Analyzer: Receives a prompt from a human UI or an MCP client and sends it to the Anthropic Claude API. It scores the prompt on 5 dimensions (clarity, token efficiency, goal alignment, structure, vagueness), identifies specific conceptual or structural mistakes, and generates an optimized rewritten prompt.
Agent 2: Analytics Reporter: Aggregates all analysis results generated by Agent 1. It calculates overall performance trends, computes agent leaderboards, and tracks mistake frequencies to provide business intelligence.
Context Store Integration: Maintains isolated, per-project context so the analyzer can provide context-aware feedback based on past prompts or specific project goals without cross-contamination.
MCP Server: Exposes these analysis capabilities as discoverable tools (
analyze_prompt
, get_analysis_history) over the Model Context Protocol (MCP) stdio for other enterprise AI agents to use autonomously.
3. Development Platform
3.1 Programming Language
The backend and agents are built entirely in Python (version 3.10+). Python was chosen for its unparalleled AI ecosystem, async capabilities, and ease of building REST endpoints and MCP servers. The frontend interfaces are written in vanilla JavaScript, HTML, and CSS.

3.2 AI Engine
The prompt analysis and scoring are powered by Anthropic's Claude AI (default model: claude-sonnet-4-20250514). Claude is utilized for its strong reasoning capabilities to critique prompts logically. The API is accessed through the official Anthropic Python SDK using strictly formatted JSON responses to guarantee parsable structures.

3.3 Database
The application uses SQLite asynchronously via aiosqlite for all data storage. The 
analytics.db
 database stores historical prompt interactions, generated scores, mistake logs, and user rewrite choices. This lightweight choice ensures easy deployment without a dedicated database server out of the box.

3.4 Web Dashboard
The web dashboard is a static HTML/JS/CSS application served by FastAPI. It relies on vanilla JavaScript to make REST API calls to the backend endpoints and updates DOM elements dynamically. It visualizes data using custom CSS layouts and integration with HTML Canvas for chart rendering.

3.5 Python Libraries
The core application relies heavily on several libraries:

fastapi & uvicorn[standard] — High-performance REST API backend and web server.
anthropic — Official Anthropic SDK for Claude API interactions.
mcp — Model Context Protocol SDK for building the stdio server.
aiosqlite — Asynchronous SQLite driver for database operations.
tiktoken — Local token counting utility to estimate efficiencies.
pydantic>=2.0 — For robust data validation and typing.
4. Infrastructure
4.1 Development Machine
Development and local testing can be achieved on any standard machine (Windows, Mac, or Linux) with Python installed. A virtual environment (.venv) is recommended for dependency management.

4.2 Production Server
The application is designed to be hosted via uvicorn on a standard VPS or containerized cloud environment. The FastAPI server handles incoming HTTP traffic and serves the static files directly.

4.3 External Services
The only external service dependency is the Anthropic Claude API. An API key (ANTHROPIC_API_KEY) is required in the environment variables to orchestrate the core analytical reasoning.

5. How It Works
5.1 Data Flow
The data flow follows an agentic pipeline. A prompt is submitted via the UI (HTTP POST) or the MCP server. The 
PromptAnalyzer
 (Agent 1) fetches contextual data for the project, structures a complex system prompt, and calls Claude. Upon receiving Claude's JSON response containing scores and a rewrite, Agent 1 passes this result to the 
AnalyticsReporter
 (Agent 2). Agent 2 persists the data into SQLite and returns a record ID. The result is then surfaced back to the user or agent client. The user's choice to accept or reject the rewrite is recorded via a follow-up API call.

5.2 Analysis Process
When triggered, 
analyzer.py
 formulates a strict instruction combining the raw prompt, the isolated project context, and the grading criteria (5 dimensions: clarity, efficiency, goal, structure, vagueness). The LLM is forced to output JSON. The application extracts this JSON, counts token differences between the raw prompt and the generated rewrite, and returns a fully typed AnalysisResult object.

5.3 Dashboard Interaction
The dashboard continuously queries the FastAPI backend (e.g., /dashboard/trends, /dashboard/interactions, /dashboard/agents) to populate its interactive UI. When an end-user navigates to the dashboard, they instantly see an overarching aggregate of token savings, a trend line of global prompt quality, a leaderboard of whichever agents/users are writing the best prompts, and a live feed of the most recent prompt analyses across the system.

6. Project Structure
6.1 Python Agent Backend
The Python architecture is modularized in multiple directories:

backend/main.py
: The FastAPI orchestrator and endpoint definitions.
prompt_analyzer/: Agent 1 logic including 
analyzer.py
, 
context_store.py
 (memory isolation), and 
anthropic_client.py
.
analytics_reporter/: Agent 2 logic prioritizing data metrics via 
reporter.py
 and 
db.py
.
mcp_server/: Standalone 
server.py
 implementation to expose Agent 1 over the Model Context Protocol.
6.2 Web Dashboard & Frontend
The user interfaces are decoupled entirely into static directories mounted by FastAPI:

frontend/
: Single-page app analyzer containing 
index.html
, 
app.js
, and 
styles.css
 for users to type prompts manually.
dashboard/
: Analytics view dashboard providing enterprise intelligence and KPIs through a dedicated interface.
7. Deployment Summary
The application is run via standard Python WSGI tooling. To instantiate the backend and UIs: uvicorn backend.main:app --reload --port 8000

This surfaces the two distinct web targets:

Analyzer: http://localhost:8000/
Dashboard: http://localhost:8000/dashboard-ui
For multi-agent integrations, the MCP Server is executed natively via Standard IO: python -m mcp_server.server

8. Cost Summary
The solution relies entirely on open-source web technologies and a local SQLite database, minimizing fixed infrastructure costs. Variable costs are tied exclusively to the Anthropic API usage. The system is configured to use claude-sonnet-4-20250514 (configurable via ANTHROPIC_MODEL), which offers a highly economical per-token rate. Due to the targeted system prompts and strict JSON requirements, overhead is minimal, allowing thousands of prompts to be evaluated for a few dollars.

End of Document
